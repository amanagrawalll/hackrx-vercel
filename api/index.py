{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# api/index.py\n\nimport os\nimport requests\nimport numpy as np\nimport groq\nfrom fastapi import FastAPI, Header, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List\nfrom io import BytesIO\nfrom pypdf import PdfReader\nfrom openai import OpenAI\n\n# --- Global Clients (loaded once) ---\nprint(\"Attempting to initialize API clients...\")\ntry:\n    # Load keys from Vercel Environment Variables\n    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n  \n    if not GROQ_API_KEY:\n        print(\"‚ö†Ô∏è GROQ_API_KEY environment variable not found.\")\n    if not OPENAI_API_KEY:\n        print(\"‚ö†Ô∏è OPENAI_API_KEY environment variable not found.\")\n\n    groq_client = groq.Groq(api_key=GROQ_API_KEY)\n    openai_client = OpenAI(api_key=OPENAI_API_KEY)\n    print(\"‚úÖ API clients initialized.\")\nexcept Exception as e:\n    groq_client = None\n    openai_client = None\n    print(f\"‚ùå Failed to initialize clients during startup: {e}\")\n\n# --- Pydantic Models ---\nclass HackRxRequest(BaseModel):\n    documents: str\n    questions: List[str]\n\nclass HackRxResponse(BaseModel):\n    answers: List[str]\n\n# --- FastAPI App ---\napp = FastAPI()\n\n# --- Helper Functions ---\ndef process_document_from_url(url: str):\n    \"\"\"Downloads a PDF from a URL, extracts text, and chunks it using a fixed-size strategy.\"\"\"\n    print(f\"‚¨áÔ∏è Downloading document from: {url}\")\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n\n        with BytesIO(response.content) as pdf_file:\n            reader = PdfReader(pdf_file)\n            text = \"\".join(page.extract_text() or \"\" for page in reader.pages)\n        \n        # --- START OF THE FIX ---\n        # A more robust chunking strategy instead of splitting by paragraph\n        \n        chunk_size = 1500  # Size of each chunk in characters\n        chunk_overlap = 200 # Number of characters to overlap between chunks\n        \n        chunks = []\n        start = 0\n        while start < len(text):\n            end = start + chunk_size\n            chunks.append(text[start:end])\n            start += chunk_size - chunk_overlap\n        \n        # Filter out any potentially empty chunks\n        chunks = [chunk for chunk in chunks if chunk.strip()]\n        # --- END OF THE FIX ---\n        \n        print(f\"üìÑ Document processed into {len(chunks)} chunks.\")\n        return chunks\n    except requests.exceptions.RequestException as e:\n        print(f\"‚ùå Error downloading or processing document: {e}\")\n        return []\n\ndef create_vector_store(chunks: list, model):\n    \"\"\"Creates a FAISS index from a list of text chunks.\"\"\"\n    if not chunks:\n        return None\n    print(\"üß† Creating embeddings and FAISS index...\")\n    embeddings = model.encode(chunks, convert_to_tensor=False, show_progress_bar=True)\n    dimension = embeddings.shape[1]\n    index = faiss.IndexFlatL2(dimension)\n    index.add(np.array(embeddings).astype('float32'))\n    print(f\"‚úÖ Vector store created with {index.ntotal} vectors.\")\n    return index\n\ndef generate_answer_with_groq(question: str, context: str):\n    \"\"\"Generates an answer using the Groq API based on the provided context.\"\"\"\n    if not client:\n        return \"Groq client not initialized. Please check API key.\"\n        \n    prompt = f\"\"\"\n    Answer the following question based ONLY on the provided context.\n    If the answer cannot be found in the context, write \"Answer not found in the provided context.\"\n    Provide a direct and concise answer.\n\n    CONTEXT:\n    {context}\n\n    QUESTION:\n    {question}\n\n    ANSWER:\n    \"\"\"\n\n    try:\n        chat_completion = client.chat.completions.create(\n            # Using Llama 3 8B model for speed and capability\n            model=\"deepseek-r1-distill-llama-70b\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            temperature=0.0,\n        )\n        return chat_completion.choices[0].message.content.strip()\n    except Exception as e:\n        print(f\"‚ùå Groq API call failed: {e}\")\n        return \"Error generating answer from Groq API.\"\n\n# ===============================================================\n# 4. API SIMULATION AND EXECUTION üöÄ\n# ===============================================================\n# --- API Endpoint ---\n@app.post(\"/hackrx-vercel/run\", response_model=HackRxResponse)\nasync def run_submission(request: HackRxRequest):\n    print(\"--- Received new request ---\")\n    try:\n        # **Critical Check:** Ensure clients are available before proceeding.\n        if not openai_client or not groq_client:\n            print(\"‚ùå Critical Error: API clients are not initialized.\")\n            raise HTTPException(status_code=500, detail=\"Server configuration error: API clients not initialized. Check environment variables.\")\n\n        # 1. Process Document\n        chunks = process_document_from_url(request.documents)\n        if not chunks: raise HTTPException(status_code=400, detail=\"Failed to process the document from the provided URL.\")\n\n        # 2. Create Embeddings for all chunks\n        print(\"Step 3: Creating embeddings for all document chunks...\")\n        chunk_embeddings = [get_embedding(chunk) for chunk in chunks]\n        print(f\"‚úÖ Embeddings created for {len(chunk_embeddings)} chunks.\")\n\n        # 3. Process each question\n        all_answers = []\n        for i, question in enumerate(request.questions):\n            print(f\"Step 4.{i+1}: Processing question: '{question[:50]}...'\")\n            question_embedding = get_embedding(question)\n\n            # 4. Find relevant context\n            similarities = [np.dot(question_embedding, chunk_emb) for chunk_emb in chunk_embeddings]\n            top_k = 5\n            top_indices = sorted(range(len(similarities)), key=lambda i: similarities[i], reverse=True)[:top_k]\n            retrieved_context = \"\\n\\n---\\n\\n\".join([chunks[i] for i in top_indices])\n            print(f\"  - Retrieved {len(top_indices)} chunks for context.\")\n\n            # 5. Generate Answer\n            answer = generate_answer_with_groq(question, retrieved_context)\n            print(f\"  - Generated answer: '{answer[:50]}...'\")\n            all_answers.append(answer)\n        \n        print(\"‚úÖ Successfully processed all questions.\")\n        return HackRxResponse(answers=all_answers)\n\n    except Exception as e:\n        print(f\"‚ùå An unexpected error occurred: {e}\")\n        raise HTTPException(status_code=500, detail=f\"An internal server error occurred: {e}\")\n\n@app.get(\"/\")\ndef read_root():\n    return {\"status\": \"API is running\"}","metadata":{"_uuid":"d1a080f7-e637-4408-aba7-0ca6adc4300c","_cell_guid":"a75a9591-6d52-4366-86b8-59bd82cdc6c2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}